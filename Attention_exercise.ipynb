{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "colab": {
   "name": "Attention_exercise.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymbPBjb6_9Ld",
    "colab_type": "text"
   },
   "source": [
    "# Week 13 Exercise - Transformers\n",
    "\n",
    "In this notebook, we will explore the creation of a Transformer Network for English to French translation.  Note that **Transformers are resource intensive and hard to train.** You will want to run these notebooks on a machine equipped with a GPU or on [Google Colab](http://colab.research.google.com).\n",
    "\n",
    "To begin, let's import a corpus of paired English and French text.  Additionally, we'll tokenize the words (i.e. create a dictionary for each vocabulary associating every word with an integer index).  There is no need to modify this cell, but have a look at what is contained in fr_to_ix (for example) and in enlines.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0xHVTA61_9Le",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "\n",
    "with open('./french.txt', encoding=\"utf-8\") as file:\n",
    "    frvocab = file.read().lower()\n",
    "    frvocab = ''.join([i if ord(i) < 128 else ' ' for i in frvocab])\n",
    "    frlines = frvocab.split('\\n')\n",
    "frlines = [re.sub(r'[^\\w\\s\\']','',i).split() for i in frlines]\n",
    "frvocab = set(re.sub(r'[^\\w\\s\\']','',frvocab).replace('\\n',' ').split(' '))\n",
    "\n",
    "with open('./english.txt', encoding=\"utf-8\") as file:\n",
    "    envocab = file.read().lower()\n",
    "    envocab = ''.join([i if ord(i) < 128 else '' for i in envocab])\n",
    "    enlines = envocab.split('\\n')\n",
    "enlines = [re.sub(r'[^\\w\\s]','',i).split() for i in enlines]\n",
    "envocab = set(re.sub(r'[^\\w\\s]','',envocab).replace('\\n',' ').strip().split(' '))\n",
    "envocab.add('<pad>')\n",
    "envocab.add('<start>')\n",
    "envocab.add('<eos>')\n",
    "frvocab.add('<pad>')\n",
    "frvocab.add('<start>')\n",
    "frvocab.add('<eos>')\n",
    "fr_to_ix = {word: i for i, word in enumerate(frvocab)}\n",
    "en_to_ix = {word: i for i, word in enumerate(envocab)}\n",
    "ix_to_fr = {fr_to_ix[word]:word for word in frvocab}\n",
    "ix_to_en = {en_to_ix[word]:word for word in envocab}\n",
    "enmax = 0\n",
    "frmax = 16\n",
    "\n",
    "for i,w in enumerate(enlines):\n",
    "    temp = len(w)\n",
    "    if temp > enmax:\n",
    "        enmax = temp\n",
    "\n",
    "for i,w in enumerate(frlines):\n",
    "    temp = len(w)\n",
    "    if temp > frmax:\n",
    "        frmax = temp"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PbZ7lvh_9Lj",
    "colab_type": "text"
   },
   "source": [
    "Next we'll create a handful of helper functions that do things like\n",
    " - Tokenize an english string, run it through the transformer producing predictions, then convert back to a french string\n",
    " - Compare predicted and target output\n",
    " - Mask a string\n",
    " - Load paired english/french sequences"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aGsqHWiZ_9Lk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def translate(sentence):\n",
    "    # Read in an english string\n",
    "    line = re.sub(r'[^\\w\\s]','',sentence).split()\n",
    "    # tokenize/pad for consistent sequence length\n",
    "    line = F.pad(torch.tensor([en_to_ix[w.lower()] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).unsqueeze(0).to(device)\n",
    "    # Create an array to hold the French sentence\n",
    "    target = torch.Tensor(1,frmax-1)\n",
    "    target = target.new_full((1,frmax-1),fr_to_ix['<pad>']).long().to(device)\n",
    "    # Start sentence with a <start> character\n",
    "    target[0,0] = fr_to_ix['<start>']\n",
    "    \n",
    "    src,trg = mask(line,target)\n",
    "    encoding = model.encode(line,src)\n",
    "    K,V = model.create_dec_KV(encoding)\n",
    "    for i in range(1,frmax-1):\n",
    "        test2 = model.decode(target,K,V,src,trg)\n",
    "        lastout = test2[0,i-1].argmax()\n",
    "        if lastout.item() == fr_to_ix['<eos>']:\n",
    "            break\n",
    "        target[0,i] = lastout\n",
    "        src,trg = mask(line,target)\n",
    "    translation = test2.argmax(2).squeeze(0)\n",
    "    translation_string = ''\n",
    "    for w in translation:\n",
    "        if ix_to_fr[w.item()] == '<eos>':\n",
    "            break\n",
    "        translation_string += ix_to_fr[w.item()] + ' '\n",
    "    return translation_string.strip()\n",
    "\n",
    "def compareoutput(preds,targetlist,loc=None):\n",
    "    # Compare model predictions with true translation\n",
    "    if loc is None:\n",
    "        loc = np.random.randint(len(preds))\n",
    "    predstr = ''\n",
    "    labelstr = ''\n",
    "    for i in range(len(preds[loc][0])):\n",
    "        if ix_to_fr[targetlist[loc][i+1].item()] == '<eos>':\n",
    "            break\n",
    "        predstr += ' '+ ix_to_fr[preds[loc][0][i].item()]\n",
    "        labelstr += ' ' + ix_to_fr[targetlist[loc][i+1].item()]\n",
    "    print(\"\\tOutput:\", predstr)\n",
    "    print(\"\\tTarget:\",labelstr)\n",
    "    \n",
    "class PositionalEncoder(nn.Module):\n",
    "    # Create a positional encoding generator\n",
    "    def __init__(self, d_model, max_seq_len = 58):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            for i in range(1,d_model,2):\n",
    "                pe[pos, i] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                \n",
    "        pe = pe\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:seq_len], \\\n",
    "        requires_grad=False).to(device)\n",
    "        return x\n",
    "\n",
    "def mask(input_seq,target_seq):\n",
    "    input_msk = (input_seq != en_to_ix['<pad>']).unsqueeze(1)\n",
    "    target_msk = (target_seq != fr_to_ix['<pad>']).unsqueeze(1)\n",
    "    size = target_seq.size(1) # get seq_len for matrix\n",
    "    nopeak_mask = np.triu(np.ones((1, size, size)),k=1)\n",
    "    nopeak_mask = Variable(torch.from_numpy(nopeak_mask).to(device) == 0)\n",
    "    target_msk = target_msk & nopeak_mask\n",
    "    return input_msk,target_msk\n",
    "\n",
    "class custdata(Dataset):\n",
    "    # Create a custom dataset object to serve up paired english and french lines\n",
    "    def __init__(self,enlines,frlines):\n",
    "        self.data_len = len(enlines) \n",
    "        self.data = [F.pad(torch.tensor([en_to_ix[w] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).to(device) for line in enlines]\n",
    "        self.labels = []\n",
    "        for line in frlines:\n",
    "            line = ['<start>',*line,'<eos>']\n",
    "            self.labels.append(F.pad(torch.tensor([fr_to_ix[w] for w in line]),(0,frmax-len(line)),value = fr_to_ix['<pad>']).to(device))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i],self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1pyh0tv_9Lo",
    "colab_type": "text"
   },
   "source": [
    "# Attention\n",
    "The first task is to code a self-attention mechanism, which corresponds to implementing Eq. 1 in Vaswani.\n",
    "#### http://jalammar.github.io/illustrated-transformer/ is a great reference for most of the programming in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4gxQ9mPH_9Lq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR Q, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.wk = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR K, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.wv = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR V, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scaler = np.sqrt(enc_dim)\n",
    "    \n",
    "    def QKV(self,x):\n",
    "        Q = self.wq(x)  #### TODO#### CALCULATE Q\n",
    "        K = self.wk(x)  #### TODO#### CALCULATE K\n",
    "        V = self.wv(x)  #### TODO#### CALCULATE V\n",
    "        return Q,K,V\n",
    "    \n",
    "    def score(self,Q,K,V,mask):\n",
    "        # scores are the stuff that goes inside the softmax\n",
    "        scores = Q @ K.T.permute(2,0,1)/self.scaler ### TODO ### CALCULATE THE SCORES. !!!DONT TOUCH THE PERMUTE!!!\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = self.dropout(F.softmax(scores,-1)) \n",
    "        return scores @ V ### TODO ### FINISH CALCULATING SELF ATTENTION\n",
    "    \n",
    "    def forward(self,x,mask=None):\n",
    "        Q,K,V = self.QKV(x)\n",
    "        return self.score(Q,K,V,mask)\n",
    "   "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H9sb24v_9Lu",
    "colab_type": "text"
   },
   "source": [
    "Next, we need to produce the \"special\" attention mechanism that takes keys and values from the encoder, but queries from the decoder.  This is very similar to the self-attention mechanism, except that there should be two inputs.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bLHXG2Jv_9Lv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class encdec_attention(nn.Module):\n",
    "    def __init__(self,dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
    "        self.wk = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
    "        self.wv = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
    "        self.scaler = np.sqrt(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def Q(self,x):\n",
    "        return self.wq(x)\n",
    "    \n",
    "    def score(self,Q,K,V,mask):\n",
    "        scores = Q @ K.T.permute(2,0,1)/self.scaler #### TODO #### SAME AS ABOVE\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = self.dropout(F.softmax(scores,-1)) \n",
    "        return scores @ V  #### TODO #### SAME AS ABOVE\n",
    "    \n",
    "    def forward(self,x,K,V,mask):\n",
    "        # DB Note: I'm not sure that this signature is right.  Seems like we should be taking x from the\n",
    "        # decoder, as well as another argument (call it y?) from the encoder, then producing K,V,Q internally,\n",
    "        # just like in the self-attention scheme.  Otherwise, how are wk and wv being used here?  it looks like \n",
    "        # these parameters have been shifted over to the Transformer module's create_dec_KV method.\n",
    "        Q = self.Q(x)\n",
    "        out = self.score(Q,K,V,mask)\n",
    "        return out"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9k_nD78_9Lz",
    "colab_type": "text"
   },
   "source": [
    "# Encoder and Decoder\n",
    "With the attention mechanisms coded, now we need to create encoder and decoder models. These correspond to the things inside the boxes in Figure 1 of Vaswani.  \n",
    "#### Fill in the forward passes of the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xMRv1vE-_9Lz",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.enc_dim = enc_dim\n",
    "        self.residual = nn.Linear(dim,enc_dim)\n",
    "        \n",
    "        self.attention = self_attention(dim,enc_dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(enc_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(enc_dim,enc_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(enc_dim)\n",
    "    \n",
    "    def forward(self,x,mask):  #### TODO #### SET UP FORWARD PASS OF ENCODER\n",
    "        x1 = self.attention.forward(x, mask)\n",
    "        if self.dim != self.enc_dim: ### DONT TOUCH, THIS IS TO HELP WITH THE RESIDUAL CONNECTION ###\n",
    "            x = self.residual(x)\n",
    "        x2 = x1 + x\n",
    "        x3 = self.norm1(x2)\n",
    "        x4 = self.linear(x3)\n",
    "        x5 = x4 + x3\n",
    "        return self.norm2(x5)\n",
    "     \n",
    "        \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.attention = self_attention(input_size,dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.EDattention = encdec_attention(dim,dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self,x,k,v,enc_mask,dec_mask):#### TODO #### SET UP FORWARD PASS OF DECODER\n",
    "        x1 = self.attention.forward(x, dec_mask)\n",
    "        x2 = x1 + x\n",
    "        x3 = self.norm1(x2)\n",
    "        x4 = self.EDattention.forward(x3, k, v, enc_mask)\n",
    "        x5 = x4 + x3\n",
    "        x6 = self.norm2(x5)\n",
    "        x7 = self.linear(x6)\n",
    "        x8 = x7 + x6\n",
    "        return self.norm3(x8)\n",
    "    "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqNSbNaA_9L3",
    "colab_type": "text"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "Build the transformer itself by hooking together encoders and decoders.  Note the word embedding layers that we are going to learn.  \n",
    "\n",
    "#### Add encoders and decoders to transformer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WbnWBv3d_9L4",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
    "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
    "        \n",
    "        self.pe1 = PositionalEncoder(dim,enmax)\n",
    "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
    "        self.encoders = []\n",
    "\n",
    "        for i in range(2):\n",
    "          self.encoders.append(encoder(dim, encoder_dim, enc_vocab_size)) #### TODO #### ADD DESIRED # OF ENCODERS TO SELF.ENCODERS\n",
    "        \n",
    "        \n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        \n",
    "        self.decoders = []\n",
    "        \n",
    "        for i in range(2):\n",
    "          self.decoders.append(decoder(encoder_dim, encoder_dim, dec_vocab_size)) #### TODO #### ADD DESIRED # OF DECODERS TO SELF.DECODERS\n",
    "\n",
    "       \n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(encoder_dim,dec_vocab_size),\n",
    "            nn.LogSoftmax(2)\n",
    "        )\n",
    "        \n",
    "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
    "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
    "    def create_dec_KV(self,z):\n",
    "        K = self.k(z)\n",
    "        V = self.v(z)\n",
    "        return K,V\n",
    "    \n",
    "    def encode(self,x,src):\n",
    "        x = self.embedding1(x)\n",
    "        x = self.pe1(x)\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x,src)\n",
    "        return x\n",
    "    \n",
    "    def decode(self,y,K,V,src,trg):\n",
    "        y = self.embedding2(y)\n",
    "        y = self.pe2(y)\n",
    "        for layer in self.decoders:\n",
    "            y = layer(y,K,V,src,trg)\n",
    "        return self.final(y)\n",
    "    \n",
    "    def forward(self,x,y,src,trg):\n",
    "        \n",
    "        x = self.encode(x,src)\n",
    "        K,V = self.create_dec_KV(x)\n",
    "        y = self.decode(y,K,V,src,trg)\n",
    "        return y"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1g_pc_7_9L8",
    "colab_type": "text"
   },
   "source": [
    "# Train the network.\n",
    "\n",
    "##### This will be slow to train and require a lot of resources. You can reduce the batch_size to lower the vram requirement, you can reduce \n",
    "##### the run time by lowering number_of_lines"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0SDFheE_9L9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
    "model = model.to(device)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eJVjevAz_9MB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
    "criterion = nn.NLLLoss()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ietfKe5C_9ME",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "BATCH_SIZE = 128\n",
    "NUMBER_OF_LINES = 20000\n",
    "\n",
    "train = custdata(enlines[:NUMBER_OF_LINES],frlines[:NUMBER_OF_LINES])\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val = custdata(enlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000],frlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000])\n",
    "valloader = torch.utils.data.DataLoader(dataset=val, batch_size=1, shuffle=True, drop_last=False)\n",
    "test = custdata(enlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000],frlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000])\n",
    "testloader = torch.utils.data.DataLoader(dataset=test, batch_size=1, shuffle=False, drop_last=False)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ln5mB534_9MI",
    "colab_type": "code",
    "outputId": "0c2203d8-cbd2-45e3-85e2-0d7157612ca0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "for i in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for j,(context, target) in enumerate(trainloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        output = model(context,trg_input,src,trg)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    scheduler.step(total_loss)\n",
    "    print('Epoch:', i+1,' loss:', total_loss)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    preds = []\n",
    "    targetlist = []\n",
    "    for j,(context, target) in enumerate(valloader):\n",
    "            trg_input = target[:,:-1]\n",
    "            targets = target.contiguous().view(-1)\n",
    "            targetlist.append(targets)\n",
    "            src,trg = mask(context,trg_input)\n",
    "            output = model(context,trg_input,src,trg)\n",
    "            pred = F.softmax(output,2).argmax(2)\n",
    "            preds.append(pred)\n",
    "            break\n",
    "    compareoutput(preds,targetlist,loc=0)"
   ],
   "execution_count": 54,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch: 1  loss: 136.90057039260864\n",
      "\tOutput:  <eos> <eos> <eos> <eos> <eos>\n",
      "\tTarget:  vous n'aviez pas le choix\n",
      "Epoch: 2  loss: 69.57477098703384\n",
      "\tOutput:  vous es <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "\tTarget:  tu ne peux pas simplement t'en aller comme a\n",
      "Epoch: 3  loss: 57.39410385489464\n",
      "\tOutput:  vous avez l'air l'air <eos>\n",
      "\tTarget:  vous n'avez aucun go t\n",
      "Epoch: 4  loss: 48.902213990688324\n",
      "\tOutput:  je je <eos>\n",
      "\tTarget:  pourquoi l'aije fait\n",
      "Epoch: 5  loss: 43.27328735589981\n",
      "\tOutput:  vous un d'une tre <eos> rifie <eos>\n",
      "\tTarget:  avezvous besoin d' tre v hicul es\n",
      "Epoch: 6  loss: 39.55209594964981\n",
      "\tOutput:  je je occuper\n",
      "\tTarget:  pourquoi l'aije fait\n",
      "Epoch: 7  loss: 37.134905859827995\n",
      "\tOutput:  c'est <eos> <eos> <eos>\n",
      "\tTarget:  waou c'est bon march\n",
      "Epoch: 8  loss: 34.7895133048296\n",
      "\tOutput:  tout r <eos>\n",
      "\tTarget:  aije manqu quiconque\n",
      "Epoch: 9  loss: 32.70316135883331\n",
      "\tOutput:  vous tes tr s avis\n",
      "\tTarget:  vous tes tr s peureux\n",
      "Epoch: 10  loss: 30.945063531398773\n",
      "\tOutput:  vous tes arr <eos>\n",
      "\tTarget:  vous tes surmen e\n",
      "Epoch: 11  loss: 29.22731128334999\n",
      "\tOutput:  quand le monde s'en <eos> aller\n",
      "\tTarget:  tout le monde devrait y aller\n",
      "Epoch: 12  loss: 21.85491820424795\n",
      "\tOutput:  laissemoi <eos>\n",
      "\tTarget:  expliquezmoi cela\n",
      "Epoch: 13  loss: 19.166944660246372\n",
      "\tOutput:  tout le monde m'a d teste\n",
      "\tTarget:  tout le monde me d teste\n",
      "Epoch: 14  loss: 17.875403828918934\n",
      "\tOutput:  le le environs\n",
      "\tTarget:  remplissez les blancs\n",
      "Epoch: 15  loss: 16.87343715876341\n",
      "\tOutput:  vous es le temps grand\n",
      "\tTarget:  tu es le plus vieux\n",
      "Epoch: 16  loss: 16.132366597652435\n",
      "\tOutput:  tesvous tes important\n",
      "\tTarget:  vous tes importants\n",
      "Epoch: 17  loss: 15.471606336534023\n",
      "\tOutput:  estu seules <eos>\n",
      "\tTarget:  tesvous seule ici\n",
      "Epoch: 18  loss: 14.862170055508614\n",
      "\tOutput:  appelle <eos> nouveau\n",
      "\tTarget:  rappelezmoi de suite\n",
      "Epoch: 19  loss: 14.357279039919376\n",
      "\tOutput:  vous es important\n",
      "\tTarget:  tu es importante\n",
      "Epoch: 20  loss: 13.879795655608177\n",
      "\tOutput:  vous vous que tu t'en ailles\n",
      "\tTarget:  il faut que tu t'en ailles\n",
      "Epoch: 21  loss: 13.38846293836832\n",
      "\tOutput:  estce femme est ici\n",
      "\tTarget:  votre femme est ici\n",
      "Epoch: 22  loss: 13.04347761720419\n",
      "\tOutput:  vous tes l' propri\n",
      "\tTarget:  vous tes le chef\n",
      "Epoch: 23  loss: 11.643316224217415\n",
      "\tOutput:  venez s'il encore\n",
      "\tTarget:  allez essaie encore\n",
      "Epoch: 24  loss: 11.406900145113468\n",
      "\tOutput:  son son son\n",
      "\tTarget:  facebook est ennuyant\n",
      "Epoch: 25  loss: 11.29403018951416\n",
      "\tOutput:  vous es trop r\n",
      "\tTarget:  tu es trop poli\n",
      "Epoch: 26  loss: 11.182123832404613\n",
      "\tOutput:  t'aije r\n",
      "\tTarget:  t'aije surpris\n",
      "Epoch: 27  loss: 11.102465007454157\n",
      "\tOutput:  ne vous qui pas\n",
      "\tTarget:  ne te fie personne\n",
      "Epoch: 28  loss: 11.060811586678028\n",
      "\tOutput:  pourquoi estu seuls\n",
      "\tTarget:  pourquoi tesvous seuls\n",
      "Epoch: 29  loss: 10.939603861421347\n",
      "\tOutput:  pourquoi estce fait\n",
      "\tTarget:  pourquoi l'avezvous fait\n",
      "Epoch: 30  loss: 10.908660639077425\n",
      "\tOutput:  taitce enfant tait t de\n",
      "\tTarget:  un enfant a besoin d'amour\n",
      "Epoch: 32  loss: 10.834393333643675\n",
      "\tOutput:  vous tes tr s timide\n",
      "\tTarget:  vous tes tr s timides\n",
      "Epoch: 33  loss: 10.740321218967438\n",
      "\tOutput:  vous le de d vieux\n",
      "\tTarget:  c'est vous le plus vieux\n",
      "Epoch: 34  loss: 10.65139202401042\n",
      "\tOutput:  tu la\n",
      "\tTarget:  aimezvous wagner\n",
      "Epoch: 35  loss: 10.585615914314985\n",
      "\tOutput:  tout fait <eos>\n",
      "\tTarget:  aije manqu quiconque\n",
      "Epoch: 36  loss: 10.556986212730408\n",
      "\tOutput:  vous aimes cie <eos> <eos> monde\n",
      "\tTarget:  tu appr cies tout le monde\n",
      "Epoch: 37  loss: 10.516862709075212\n",
      "\tOutput:  vous tes faites de\n",
      "\tTarget:  vous me faites mal\n",
      "Epoch: 38  loss: 10.411112263798714\n",
      "\tOutput:  tu sais peux pas le dire\n",
      "\tTarget:  tu ne peux pas le manquer\n",
      "Epoch: 39  loss: 10.37511820718646\n",
      "\tOutput:  vous es fort contrari <eos>\n",
      "\tTarget:  tu es fort contrari e\n",
      "Epoch: 40  loss: 10.33001758158207\n",
      "\tOutput:  vous es d gueulasse t\n",
      "\tTarget:  tu es d go tant\n",
      "Epoch: 41  loss: 10.250234678387642\n",
      "\tOutput:  qui est votre p re\n",
      "\tTarget:  qui est votre p re\n",
      "Epoch: 42  loss: 10.212941035628319\n",
      "\tOutput:  tesvous sourde ress e\n",
      "\tTarget:  tesvous int ress s\n",
      "Epoch: 43  loss: 10.161260411143303\n",
      "\tOutput:  tu aimes bien <eos> le monde\n",
      "\tTarget:  tu aimes bien tout le monde\n",
      "Epoch: 44  loss: 10.110875248908997\n",
      "\tOutput:  suivez en r placement\n",
      "\tTarget:  suis ton d sir\n",
      "Epoch: 46  loss: 9.954859223216772\n",
      "\tOutput:  tesvous es d\n",
      "\tTarget:  tu es press\n",
      "Epoch: 48  loss: 9.860689226537943\n",
      "\tOutput:  d jusque <pad>\n",
      "\tTarget:  compte jusqu' trente\n",
      "Epoch: 49  loss: 9.83701241016388\n",
      "\tOutput:  vous tes fort s timide\n",
      "\tTarget:  vous tes tr s craintif\n",
      "Epoch: 50  loss: 9.775578651577234\n",
      "\tOutput:  tesvous heureux <eos>\n",
      "\tTarget:  estu heureux ici\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm97Noqf_9ML",
    "colab_type": "text"
   },
   "source": [
    "# Test your translator\n",
    "\n",
    "##### Unless you speak french you're going have to check it with google translate https://translate.google.com/\n",
    "##### I found it started doing alright once the loss got below 10 but this might take a while"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ITVFWntl_9MM",
    "colab_type": "code",
    "outputId": "36c7cc46-0da7-44af-ab24-40165c2d6052",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "sentence = 'how are you'\n",
    "translate(sentence)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'comment vastu'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtVxgmk4_9MQ",
    "colab_type": "text"
   },
   "source": [
    "#### Test it on testing data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mJvoH2Ly_9MQ",
    "colab_type": "code",
    "outputId": "614c3660-bb79-45a6-d18f-f9022885dbbb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    }
   },
   "source": [
    "model.eval()\n",
    "scores = []\n",
    "preds = []\n",
    "targetlist = []\n",
    "for j,(context, target) in enumerate(testloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        targetlist.append(targets)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        output = model(context,trg_input,src,trg).detach()\n",
    "        pred = F.softmax(output,2).argmax(2)\n",
    "        preds.append(pred)\n",
    "        correct = sum(pred[0][targets!=fr_to_ix['<pad>']]==targets[targets!=fr_to_ix['<pad>']]).item()/len(targets[targets!=fr_to_ix['<pad>']])\n",
    "        scores.append(correct)\n",
    "plt.plot(scores)\n",
    "print('Average # of words correct',np.mean(scores))"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Average # of words correct 0.6050322510822511\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": [],
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4Z3LqNt_9MU",
    "colab_type": "text"
   },
   "source": [
    "# MULTI-HEAD ATTENTION (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NIc0VZmW_9MV",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "class multi_attention(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        \n",
    "        heads = []\n",
    "        for i in range(5): ### TODO ### CHOOSE THE # OF HEADS YOU WANT\n",
    "            heads.append(self_attention(dim, encoder_dim)) ### TODO ### ADD SELF_ATTENTION LAYERS TO HEADS\n",
    "        \n",
    "        self.heads = nn.ModuleList(heads)\n",
    "        \n",
    "        self.linear = nn.Linear((len(heads) * encoder_dim),encoder_dim) ### TODO ###\n",
    "    \n",
    "    \n",
    "    def forward(self,x,mask=None):\n",
    "        headoutputs = [layer(x,mask) for layer in self.heads]\n",
    "        headoutputs = torch.cat(headoutputs,dim=2)\n",
    "        return self.linear(headoutputs)\n",
    "    \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.enc_dim = enc_dim\n",
    "        self.attention = multi_attention(dim,enc_dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(enc_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(enc_dim,enc_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.residual = nn.Linear(dim,enc_dim)\n",
    "        self.norm2 = nn.LayerNorm(enc_dim)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        z = self.attention(x,mask)\n",
    "        if self.dim != self.enc_dim:\n",
    "            x = self.residual(x)\n",
    "        z = self.norm1(x+z)\n",
    "        z2 = self.linear(z)\n",
    "        return self.norm2(z+z2)\n",
    "     \n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.attention = multi_attention(input_size,dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.EDattention = encdec_attention(dim,dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self,x,k,v,src,trg):\n",
    "        z = self.attention(x,trg)\n",
    "        z = self.norm1(z+x)\n",
    "        z2 = self.EDattention(z,k,v,src)\n",
    "        z2 = self.norm2(z2+z)\n",
    "        z3 = self.linear(z2)\n",
    "        return self.norm3(z3+z2)\n",
    "    \n",
    "class transformer(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
    "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
    "        \n",
    "        self.pe1 = PositionalEncoder(dim,enmax)\n",
    "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
    "        \n",
    "        self.encoders = []\n",
    "        for i in range(2):\n",
    "          self.encoders.append(encoder(dim,encoder_dim,enc_vocab_size))   ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        \n",
    "        self.decoders = []\n",
    "        for i in range(2):\n",
    "          self.decoders.append(decoder(encoder_dim,encoder_dim,dec_vocab_size)) ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(encoder_dim,dec_vocab_size),\n",
    "            nn.LogSoftmax(2)\n",
    "        )\n",
    "        \n",
    "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
    "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
    "        \n",
    "    def create_dec_KV(self,z):\n",
    "        K = self.k(z)\n",
    "        V = self.v(z)\n",
    "        return K,V\n",
    "    \n",
    "    def encode(self,x,src):\n",
    "        x = self.embedding1(x)\n",
    "        x = self.pe1(x)\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x,src)\n",
    "        return x\n",
    "    \n",
    "    def decode(self,y,K,V,src,trg):\n",
    "        y = self.embedding2(y)\n",
    "        y = self.pe2(y)\n",
    "        for layer in self.decoders:\n",
    "            y = layer(y,K,V,src,trg)\n",
    "        return self.final(y)\n",
    "        \n",
    "    \n",
    "    def forward(self,x,y,src,trg):\n",
    "        x = self.encode(x,src)\n",
    "        K,V = self.create_dec_KV(x)\n",
    "        y = self.decode(y,K,V,src,trg)\n",
    "        \n",
    "        return y\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yyr53dWr_9MY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
    "model = model.to(device)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gfhrAIHh_9Mb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
    "criterion = nn.NLLLoss()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "ZGWiZoO1_9Mf",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "6b5fc930-11cf-4469-9917-c5eecdbe06e6"
   },
   "source": [
    "for i in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for j,(context, target) in enumerate(trainloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        output = model(context,trg_input,src,trg)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    scheduler.step(total_loss)\n",
    "    print('Epoch:', i+1,' loss:', total_loss)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    preds = []\n",
    "    targetlist = []\n",
    "    for j,(context, target) in enumerate(valloader):\n",
    "            trg_input = target[:,:-1]\n",
    "            targets = target.contiguous().view(-1)\n",
    "            targetlist.append(targets)\n",
    "            src,trg = mask(context,trg_input)\n",
    "            output = model(context,trg_input,src,trg)\n",
    "            pred = F.softmax(output,2).argmax(2)\n",
    "            preds.append(pred)\n",
    "            break\n",
    "    compareoutput(preds,targetlist,loc=0)"
   ],
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch: 1  loss: 137.89631870388985\n",
      "\tOutput:  <eos> <eos> <eos> <eos>\n",
      "\tTarget:  tu as l'air japonaise\n",
      "Epoch: 2  loss: 78.5301513671875\n",
      "\tOutput:  <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "\tTarget:  ne le dis qui que ce soit\n",
      "Epoch: 3  loss: 73.81346639990807\n",
      "\tOutput:  il est est <eos> <eos>\n",
      "\tTarget:  tout le monde l'appr cie\n",
      "Epoch: 4  loss: 67.04909181594849\n",
      "\tOutput:  tom <eos> <eos>\n",
      "\tTarget:  vide tes poches\n",
      "Epoch: 5  loss: 60.789752304553986\n",
      "\tOutput:  tu es\n",
      "\tTarget:  estu courageuse\n",
      "Epoch: 6  loss: 57.194464802742004\n",
      "\tOutput:  vous es tes <eos> <eos>\n",
      "\tTarget:  tu es tr s craintif\n",
      "Epoch: 7  loss: 55.67516142129898\n",
      "\tOutput:  le le <eos>\n",
      "\tTarget:  tout est parti\n",
      "Epoch: 8  loss: 53.50828978419304\n",
      "\tOutput:  tu avez l'air <eos>\n",
      "\tTarget:  vous avez l'air resplendissantes\n",
      "Epoch: 9  loss: 50.35978338122368\n",
      "\tOutput:  vous es tr s\n",
      "\tTarget:  tu es fort craintif\n",
      "Epoch: 10  loss: 48.10208210349083\n",
      "\tOutput:  tu que que train <eos>\n",
      "\tTarget:  c'est vous la ma tresse\n",
      "Epoch: 11  loss: 46.76873815059662\n",
      "\tOutput:  vous en <eos> bonne\n",
      "\tTarget:  c'est vous le doyen\n",
      "Epoch: 12  loss: 40.743137046694756\n",
      "\tOutput:  vous es en en\n",
      "\tTarget:  tu es l'a n\n",
      "Epoch: 13  loss: 38.6273325830698\n",
      "\tOutput:  peuxtu peux aider peut\n",
      "\tTarget:  tu peux t'y fier\n",
      "Epoch: 14  loss: 37.41244924068451\n",
      "\tOutput:  il a un verre\n",
      "\tTarget:  qui fautil un verre\n",
      "Epoch: 15  loss: 36.39142596721649\n",
      "\tOutput:  tu que tu que <eos>\n",
      "\tTarget:  estce que tu aimes moscou\n",
      "Epoch: 16  loss: 35.52949145436287\n",
      "\tOutput:  vous tes tr tr\n",
      "\tTarget:  vous tes fort courageuse\n",
      "Epoch: 17  loss: 34.652748480439186\n",
      "\tOutput:  vous es tr s occup\n",
      "\tTarget:  tu es tr s riche\n",
      "Epoch: 18  loss: 33.90327453613281\n",
      "\tOutput:  ne fais pas <eos> <eos>\n",
      "\tTarget:  ne t'approche pas de moi\n",
      "Epoch: 19  loss: 33.10963875055313\n",
      "\tOutput:  je ne pas <eos> <eos> vous\n",
      "\tTarget:  vous n'aurez pas besoin de moi\n",
      "Epoch: 20  loss: 32.342741921544075\n",
      "\tOutput:  vous es fort s avis col s\n",
      "\tTarget:  tu es tr s en col re\n",
      "Epoch: 21  loss: 31.601434290409088\n",
      "\tOutput:  comment va tout patron <eos> tom\n",
      "\tTarget:  tu es le fils de qui\n",
      "Epoch: 22  loss: 30.852292075753212\n",
      "\tOutput:  vous tous s <eos>\n",
      "\tTarget:  estu int ress e\n",
      "Epoch: 23  loss: 29.288832664489746\n",
      "\tOutput:  vous avez l'air tort <eos>\n",
      "\tTarget:  vous avez l'air europ en\n",
      "Epoch: 24  loss: 28.98957370221615\n",
      "\tOutput:  qui doit besoin de <eos>\n",
      "\tTarget:  qui t'a envoy e ici\n",
      "Epoch: 25  loss: 28.830994188785553\n",
      "\tOutput:  viens <eos> rieur\n",
      "\tTarget:  rappellemoi de suite\n",
      "Epoch: 26  loss: 28.682079195976257\n",
      "\tOutput:  fais comme tu vous bon\n",
      "\tTarget:  fais comme il te dit\n",
      "Epoch: 27  loss: 28.57871024310589\n",
      "\tOutput:  vous tes tr s avis\n",
      "\tTarget:  vous tes tr s peureux\n",
      "Epoch: 28  loss: 28.454462319612503\n",
      "\tOutput:  vous tes tr s\n",
      "\tTarget:  vous tes fort riches\n",
      "Epoch: 29  loss: 28.316127106547356\n",
      "\tOutput:  la v <eos> <eos> <eos>\n",
      "\tTarget:  les bois br lent facilement\n",
      "Epoch: 30  loss: 28.244946539402008\n",
      "\tOutput:  vous tes en\n",
      "\tTarget:  vous tes productive\n",
      "Epoch: 31  loss: 28.112772822380066\n",
      "\tOutput:  tu tes trop <eos> <eos>\n",
      "\tTarget:  vous tes venue trop tard\n",
      "Epoch: 32  loss: 28.01653192937374\n",
      "\tOutput:  puisje aider <eos>\n",
      "\tTarget:  puisje voir celuici\n",
      "Epoch: 33  loss: 27.867065235972404\n",
      "\tOutput:  tout est va bonne\n",
      "\tTarget:  tout a son importance\n",
      "Epoch: 34  loss: 27.80839830636978\n",
      "\tOutput:  voici la vin\n",
      "\tTarget:  bas le roi\n",
      "Epoch: 35  loss: 27.66327676177025\n",
      "\tOutput:  vous n' tes pas arm\n",
      "\tTarget:  vous n' tes pas normales\n",
      "Epoch: 36  loss: 27.558620750904083\n",
      "\tOutput:  attends une minute\n",
      "\tTarget:  excusezmoi une minute\n",
      "Epoch: 37  loss: 27.434070974588394\n",
      "\tOutput:  quiconque atil qui\n",
      "\tTarget:  pourquoi c'est marrant\n",
      "Epoch: 38  loss: 27.322979897260666\n",
      "\tOutput:  vous es es bien\n",
      "\tTarget:  tu me fais mal\n",
      "Epoch: 39  loss: 27.170584306120872\n",
      "\tOutput:  voici mon chapeau\n",
      "\tTarget:  suivez son exemple\n",
      "Epoch: 40  loss: 27.085332915186882\n",
      "\tOutput:  vous tes tes bien\n",
      "\tTarget:  vous me faites mal\n",
      "Epoch: 41  loss: 26.985174655914307\n",
      "\tOutput:  vous t'aider prouver\n",
      "\tTarget:  arrivestu le croire\n",
      "Epoch: 42  loss: 26.86986108124256\n",
      "\tOutput:  tu n' pouvez pas <eos> <eos>\n",
      "\tTarget:  vous ne serez pas fusill e\n",
      "Epoch: 43  loss: 26.774294167757034\n",
      "\tOutput:  venez de vous\n",
      "\tTarget:  viens avec moi\n",
      "Epoch: 44  loss: 26.660575568675995\n",
      "\tOutput:  vous tes tr s s\n",
      "\tTarget:  vous tes tr s riche\n",
      "Epoch: 45  loss: 26.509419679641724\n",
      "\tOutput:  tu pouvez y <eos>\n",
      "\tTarget:  vous pouvez entrer maintenant\n",
      "Epoch: 46  loss: 26.409744575619698\n",
      "\tOutput:  vous l' la vie vie\n",
      "\tTarget:  c'est toi la ma tresse\n",
      "Epoch: 47  loss: 26.29068386554718\n",
      "\tOutput:  ne vous pas <eos>\n",
      "\tTarget:  ne m'avezvous pas entendu\n",
      "Epoch: 48  loss: 26.164037704467773\n",
      "\tOutput:  tout est <eos>\n",
      "\tTarget:  tout va bien\n",
      "Epoch: 49  loss: 26.07014763355255\n",
      "\tOutput:  estu <eos>\n",
      "\tTarget:  partezvous aussi\n",
      "Epoch: 50  loss: 25.9716068059206\n",
      "\tOutput:  tout le monde a monde\n",
      "\tTarget:  tout le monde le savait\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5h1Ksmr_9Mi",
    "colab_type": "text"
   },
   "source": [
    "# Test your  multi-head transformer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "trOyQbdf_9Mj",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "b2e850ba-fd87-4559-b3ab-523f344baa25"
   },
   "source": [
    "sentence = 'how are you'\n",
    "translate(sentence)"
   ],
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'comment va'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 44
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKtWO8VJ_9Mm",
    "colab_type": "text"
   },
   "source": [
    "# QUESTION\n",
    "\n",
    "## 1) Was the runtime of your multi-head transformer noticably longer than the single head one? What about the speed the loss decreased? If you had the time and resources to train it to a good spot, how did the translation quality compare to the single-headed transformer?\n",
    "\n",
    "### Answer\n",
    "---\n",
    "Adding a 5 headed multi attention certainly took longer did not seem to significantly increase the training time over \n",
    "the single head with 2 encoders and 2 decoders for both. It also seems to be converging much slower. This likely due to \n",
    "the increased expressiveness, ie more parameters to optimize. \n",
    "\n",
    "## 2)Try adding encoders and decoders to one of your transformers. Does having the extra layers improve performance? How does it affect runtime?\n",
    "\n",
    "### Answer\n",
    "---\n",
    "Adding additional encoders and decoders did increase the runtime significantly. I settled on having 2 encoders and 2 \n",
    "decoders for a balance of accuracy and runtime. On the high end I tried 5 of each and on the low end 1 of each. The \n",
    "networks with a high number of encoders/decoders also converged slower just as the milti-head network did because of \n",
    "the increased number of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yUAFzg8U_9Mm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}